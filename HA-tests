HA tests
========

Schroedinger maintenance, 25 june 2014
--------------------------------------

Since
- STONITH was disabled so far
- `ping` primitive was not working properly
- we need to test if the following Infiniband ethmonitor timeout is working properly
	- short answer: no, I think timeout should be less than interval * repeat_count, where the default for repeat_count is 5

	::
	
		primitive ib0_up ethmonitor \
				params interface=ib0 name=ib0_up \
				op monitor interval=5s timeout=60s \
				   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
				op start interval=0 timeout=60s \
				op stop interval=0	


I propose to re-do all the tests we did during last maintenance.

TO-DO tests
^^^^^^^^^^^


Goal of the tests is:
	- verify STONITH is working well
	- verify ethmonitor on InfiniBand is working well (timeout issue)
	- verify the expected behaviour when a node experiments a failure

**Expected behaviour**: *resources (Lustre target) are migrated correctly on partner node. After re-connecting the cable, the resource is NOT migrated back automatically*.
	

To accomplish this we need:

	to enable STONITH
		- crm configure property stonith-enabled=true
		- when STONITH is enabled, the default behaviour for a failing Pacemaker resource is fencing
	if needed, modify the timeout value in the ethmonitor resource
		- we need a timeout smaller than interval*repeat_count
		- for instance ("...op monitor interval=10s timeout=50s...")



Single host failures
""""""""""""""""""""


- FC
	- unlink first cable
		nothing must happen, multipath acknowledges the missing path
	- unlink second cable
		multipath acknowledges no path to volume
		filesystem primitive acknowledges impossible I/O on disk (thanks to CHECK_OCF_LEVEL=10)
		filesystem primitive fails
		failing node is fenced (power off)
		Pacemaker migrates Lustre target
		
- IB
	- unlink cable
		ethmonitor acknowledges missing link (before ping primitive)
		ethmonitor fails
		failing node is fenced (power off)
		Pacemaker migrates Lustre target
		
	- do we need to run again the disable IB port test (ibportstate)?
		...
		
- ethernet
	- unlink cable
		Pacemaker must continue operations thanks to redundant rings
		STONITH resource cannot operate, so will fail?
		node will be fenced? we don't need fence here.. just move resources?
			to avoid this behaviour, we need to modify the default for the stop operation (no "on-fail=fence")
		



Tests already performed
=======================

Expected behaviour definition: *resources are migrated correctly on pair node. After re-connecting the cable, the resource is NOT migrated back automatically*.

Single host failures
--------------------

- FC (lustre-mds2, mgt target)
	- unlink first cable
		- behaviour: as expected
	- unlink second one
		- behaviour: as expected.


- IB (lustre-oss2)

    - unlink cable
        - behaviour: as expected.

    - disable IB port
        - behaviour: as expected.

    - block ICMP
        - behaviour ? > RM: I think this test can be safely skipped!

        
- oss2 ethernet (do we have another interface available on hosts, and ports on switch?)
    - unlink cable
      - Apparently, this cause corosync communication to stop (even
        though they should talk on the IB channel), and all resources
        to be blocked. resources are stopped on the "dead" node, but
        not migrated to other resources.
      - replacing the cable, something strange happens: resources are
        first mounted on oss1 (the pair resource), and *then* migrated
        to oss2 (the "dead" node) . Misbehaviour may be related to the ``ibportstate`` command.
        
- ethernet (lustre-mds2)
    - unlink cable
      - ping fails because the machine cannot resolve ib* hostnames,
        and mgt is moved to lustre-mds1.
    - expected behaviour: the resource should not migrate due to the presence of the second communication ring (if the ping resource is working).


Multiple hosts failure
----------------------

> RM: In both these cases, I think the Lustre storage cluster should
> be down.  At any rate, these are failure modes that cannot be served
> by our current HW configuration!

- what if both the mds are down?
    - do we need another check on pacemaker?

- what about the quorum? if 5 hosts are down?
    - behaviour? 



Open points
-----------

* STONITH is still disabled on all servers

* eth1 on servers is not used. We could:
	- bond it with eth0 (do we have another port on switch?)
	- link it to the IPMI LAN?

* What if eth0.617 is not available?

* InfiniBand behaviour	
	- ethmonitor timeout
		- looking at ``crm ra info ocf:heartbeat:ethmonitor``, the timeout should be `at least repeat_interval \* repeatcount`. But the default is 20 seconds.
	- when InfiniBand is missing, do we want a failover caused by failure of ping or ethmonitor resource?
		- adjust timeout as needed
	- when InfiniBand is missing, what behaviour do we expect?
		- do we need fencing?
		
* Quorum: we have ten nodes in the HA cluster
	- at least N/2+1 nodes are needed to have quorum
	- when 5 nodes can't vote, what's happening?
	
* What if both the MDS servers are missing?
	- since we have a Mandatory orded (``order mdt_after_mgt Mandatory: mgt mdt``), a missing mgt should actually lead every Lustre target down

* Why the ``interleave`` meta attribute in the ethmonitor clone for InfiniBand check?
	- the interleave is needed `only when a master/slave set is configured <http://www.hastexo.com/resources/hints-and-kinks/interleaving-pacemaker-clones>`__
	

* Ping is failing when eth0 is missing (and then resource will migrate)
	- change hostnames with ip addresses?
	- name resolution handling
	
No failures (maintenance use cases)
-----------------------------------

- Set the HA cluster in *maintenance-mode*
	``crm configure property maintenance-mode=true``

- Tell CRM to migrate services (expect: services migrate)
	use ``crm resource migrate [resource-name] [FQDN]`` to move the resource TO the FQDN.

- Clean shutdown of a host (expect: services migrate)
	I would not perform a node shutdown with Pacemaker running on that node. I would migrate the services, and then cleanly shut down the node.
	If for any reason a node fails to move target to its partner, this last one will fence the node.

- Stop CRM on node (expect: services migrate)
	CRM is not a daemon: is the shell you use to interact with Pacemaker. You could stop ``corosync`` or ``pacemaker``.

	
	
Tests on lustre-test{1,2}
-------------------------

test HA
	- unlink first SAS, ok
	- unlink second SAS
		- fail of filesystem monitoring
		- trying to stop target on failing node
		- timeout on stop operation -> FENCING
	
	- when machine reboots, still SAS unlinked
		- node resources unmounted
		- trying to start target
		
		
	- ib test
		- physically unlink ib0
			- migration OK
			- failback OK when relink
		- ifdown ib0
			- migration OK
			- failback OK when ifup
		- why no stonith?
			- timeout need to be less then repeat_count * repeat_interval ?


