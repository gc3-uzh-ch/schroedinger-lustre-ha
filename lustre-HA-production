lustre HA - production servers
=============================

- lsof /lustre on clients?
- umount all lustre target
    - shine stop
    - lustre_rmmod

- read only on SAN: everything but MGT ?
    - copy MGT block device, somewhere
    - multipath: flush devices
        - multipath -F everywhere
    - read only on SAN
        - # unmap volume ost24
        - # map volume access ro lun 18 ost24
    - read write for MGT on SAN
        - ok
    - multipath reload
        - reboot is needed to have wp=ro flag and
        - #  cat /sys/block/sdk/ro # -> 1
    - check: 
    # pdsh -g oss,mds 'multipath -ll' 2> /dev/null | grep "wp=ro"  | sort | uniq -c
      1 lustre-mds1: size=559G features='1 queue_if_no_path' hwhandler='0' wp=ro
      1 lustre-mds2: size=559G features='1 queue_if_no_path' hwhandler='0' wp=ro
      8 lustre-oss2: size=7.3T features='1 queue_if_no_path' hwhandler='0' wp=ro
      8 lustre-oss3: size=7.3T features='1 queue_if_no_path' hwhandler='0' wp=ro
      8 lustre-oss4: size=7.3T features='1 queue_if_no_path' hwhandler='0' wp=ro
      8 lustre-oss5: size=7.3T features='1 queue_if_no_path' hwhandler='0' wp=ro
      8 lustre-oss6: size=7.3T features='1 queue_if_no_path' hwhandler='0' wp=ro
      8 lustre-oss7: size=7.3T features='1 queue_if_no_path' hwhandler='0' wp=ro
      8 lustre-oss8: size=7.3T features='1 queue_if_no_path' hwhandler='0' wp=ro
    # pdsh -g oss,mds 'multipath -ll' 2> /dev/null | grep "wp=rw"  | sort | uniq -c
      1 lustre-mds1: size=173M features='1 queue_if_no_path' hwhandler='0' wp=rw
      1 lustre-mds2: size=173M features='1 queue_if_no_path' hwhandler='0' wp=rw
      
    
- copy packages on servers
    - # pdcp -g oss,mds -r rpmbuild/ /tmp/rpmbuild
    - # pdsh -g oss,mds 'cd /tmp/rpmbuild/RPMS/x86_64 && yum -y localinstall corosync-2.3.3-1.el6.x86_64.rpm corosynclib-2.3.3-1.el6.x86_64.rpm pacemaker-1.1.12-1.el6.x86_64.rpm pacemaker-cli-1.1.12-1.el6.x86_64.rpm pacemaker-cluster-libs-1.1.12-1.el6.x86_64.rpm pacemaker-libs-1.1.12-1.el6.x86_64.rpm cluster-glue-1.0.12-0.rc1.el6.x86_64.rpm cluster-glue-libs-1.0.12-0.rc1.el6.x86_64.rpm'
    - # pdsh -g oss,mds 'yum install fence-agents'    
    
- corosync conf (from lustre-test1)
    - root@gw2:~/eXact/corosync2.3.3# pdcp -g oss,mds corosync.conf /etc/corosync/corosync.conf
    
- crmsh, from suse repo
    - root@gw2:~/eXact/corosync2.3.3# wget http://download.opensuse.org/repositories/network:/ha-clustering:/Stable/CentOS_CentOS-6/network:ha-clustering:Stable.repo
    - root@gw2:~/eXact/corosync2.3.3# pdcp -g oss,mds network\:ha-clustering\:Stable.repo  /etc/yum.repos.d/

- start corosync, pacemaker
    - pdsh -g oss,mds 'service corosync start'
    - pdsh -g oss,mds 'service pacemaker start'
    - check if running
    
- erase everything before start (will DELETE EVERY resources previously configured)
    - crm configure erase




Final configuration for production
----------------------------------

[root@lustre-test1 lustre]# cat final_working
node 176244254: lustre-test1
node 176244255: lustre-test2 \
	attributes standby=off
primitive NIC_CHECK_IB ethmonitor \
	params interface=ib0 name=NIC_CHECK_IB \
	op monitor interval=20 timeout=40 on-fail=fence multiplier=200000 \
	op start timeout=120 interval=0 \
	op stop interval=0 timeout=20 on-fail=fence
primitive mdt Filesystem \
	params device="/dev/mapper/ha_volp3" directory="/srv/lustre/mdt/0" fstype=lustre \
	meta target-role=Started \
	operations $id="mdt-operations" \
	op monitor interval=120 timeout=60 OCF_CHECK_LEVEL=10 \
	op start interval=0 timeout=300 \
	op stop interval=0 timeout=300 on-fail=fence
primitive mgt Filesystem \
	params device="/dev/mapper/ha_volp4" directory="/srv/lustre/mgt" fstype=lustre \
	meta target-role=Started \
	operations $id="mgt-operations" \
	op monitor interval=120 timeout=60 OCF_CHECK_LEVEL=10 \
	op start interval=0 timeout=300 \
	op stop interval=0 timeout=300 on-fail=fence
primitive ost1 Filesystem \
	params device="/dev/mapper/ha_volp1" directory="/srv/lustre/ost/0" fstype=lustre \
	meta target-role=Started \
	operations $id="ost1-operations" \
	op monitor interval=120 timeout=60 OCF_CHECK_LEVEL=10 \
	op start interval=0 timeout=300 \
	op stop interval=0 timeout=300 on-fail=fence
primitive ost2 Filesystem \
	params device="/dev/mapper/ha_volp2" directory="/srv/lustre/ost/1" fstype=lustre \
	meta target-role=Started \
	operations $id="ost2-operations" \
	op monitor interval=120 timeout=60 OCF_CHECK_LEVEL=10 \
	op start interval=0 timeout=300 \
	op stop interval=0 timeout=300 on-fail=fence
primitive stonith-lustre-test1 stonith:fence_ipmilan \
	params pcmk_host_list=lustre-test1 pcmk_host_check=static-list ipaddr=10.128.70.30 login=root passwd="changeme!now" verbose=true lanplus=true power_wait=4 \
	op monitor interval=60s
primitive stonith-lustre-test2 stonith:fence_ipmilan \
	params pcmk_host_list=lustre-test2 pcmk_host_check=static-list ipaddr=10.128.70.31 login=root passwd="changeme!now" verbose=true lanplus=true power_wait=4 \
	op monitor interval=60s
clone NIC_CHECK_IB_CLONE NIC_CHECK_IB \
	meta globally-unique=false
location cli-prefer-mdt mdt \
	rule $id="mdt_on_test1" inf: #uname eq lustre-test1 \
	rule $id="mdt_only_on_ib" -inf: not_defined NIC_CHECK_IB or NIC_CHECK_IB lte 0
location cli-prefer-mgt mgt \
	rule $id="mgt_on_test2" inf: #uname eq lustre-test2 \
	rule $id="mgt_only_on_ib" -inf: not_defined NIC_CHECK_IB or NIC_CHECK_IB lte 0
location cli-prefer-ost1 ost1 \
	rule $id="ost1_on_test1" inf: #uname eq lustre-test1 \
	rule $id="ost1_only_on_ib" -inf: not_defined NIC_CHECK_IB or NIC_CHECK_IB lte 0
location cli-prefer-ost2 ost2 \
	rule $id="ost2_on_test2" inf: #uname eq lustre-test2 \
	rule $id="ost2_only_on_ib" -inf: not_defined NIC_CHECK_IB or NIC_CHECK_IB lte 0
location lc-stonith-lustre-test1 stonith-lustre-test1 -inf: lustre-test1
location lc-stonith-lustre-test2 stonith-lustre-test2 -inf: lustre-test2
property cib-bootstrap-options: \
	dc-version=1.1.11-b434fee \
	cluster-infrastructure=corosync \
	stonith-enabled=true \
	no-quorum-policy=ignore \
	maintenance-mode=false \
	last-lrm-refresh=1400683362